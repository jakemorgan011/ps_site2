<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>horse.beer/dimensionality</title>
    <link rel="stylesheet" href="src/css/style.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"></script>
    <!-- not sure how i feel about this -->
    <script src="src/lib/dsp.js"></script>
</head>
<body>
    <h1><a href="index.html">horse.beer</a>/dimensionality</h1>
    <div class='content' id='dimensionality'>
        <div class='sidebar'>
            <h2 style="border: wave; font-size: 1.76em;">controls</h2>
            
            <p class='ani-bar' id='p0' style="font-size: 1.35em; margin: 0;">--------</p>
            
            <label for="obj-input">
                <i class="fas fa-upload"></i>upload .obj
            </label>
            <input type="file" id="obj-input" accept=".obj" />
            
            <label for="wav-input">
                <i class="fas fa-upload"></i>upload .wav
            </label>
            <input type="file" id="wav-input" accept=".wav,.mp3" />
            
            <p class='ani-bar' id='p1' style="font-size: 1.35em; margin: 0;">----------</p>
            
            <button id="process-btn" disabled>process audio</button>
            <button id="play-btn" disabled>start playback</button>
            <button id="stop-btn" disabled>stop playback</button>
            
            <p class='ani-bar' id='p2' style="font-size: 1.35em; margin: 0;">----------</p>
            
            <button id="download-btn" disabled>download .wav</button>
            
            <div id="status" style="color: white; padding: 10px; margin-top: 20px;">
                <h3>Status:</h3>
                <p id="status-text">Ready</p>
            </div>
            
            <div id="params-display" style="color: white; padding: 10px; margin-top: 20px; display: none;">
                <h3>Mesh Parameters:</h3>
                <div id="params-list"></div>
            </div>
        </div>
        
        <div class='sketch-container' style="justify-content: center;">
            <div class='canvas-container' id='canvas-container-dimensionality'>
                <script src="src/js/sketch2.js"></script>
            </div>
        </div>
    </div>
    
    <script src="src/js/p_animate.js"></script>
    
    <script>
    class OBJParser {
        static parse(text) {
            const vertices = [];
            const lines = text.split('\n');
            
            for (const line of lines) {
                const parts = line.trim().split(/\s+/);
                if (parts[0] === 'v') {
                    vertices.push({
                        x: parseFloat(parts[1]),
                        y: parseFloat(parts[2]),
                        z: parseFloat(parts[3])
                    });
                }
            }
            
            return vertices;
        }
    }
    
    class MeshFingerprint {
        static calculate(vertices) {
            if (vertices.length === 0) return null;
            
            const byX = [...vertices].sort((a, b) => a.x - b.x);
            const byY = [...vertices].sort((a, b) => a.y - b.y);
            const byZ = [...vertices].sort((a, b) => a.z - b.z);
            const byDistance = [...vertices].sort((a, b) => {
                const distA = Math.sqrt(a.x**2 + a.y**2 + a.z**2);
                const distB = Math.sqrt(b.x**2 + b.y**2 + b.z**2);
                return distA - distB;
            });
            
            const stats = {
                x10: byX[Math.floor(vertices.length * 0.1)].x,
                x50: byX[Math.floor(vertices.length * 0.5)].x,
                x90: byX[Math.floor(vertices.length * 0.9)].x,
                
                y10: byY[Math.floor(vertices.length * 0.1)].y,
                y50: byY[Math.floor(vertices.length * 0.5)].y,
                y90: byY[Math.floor(vertices.length * 0.9)].y,
                
                z10: byZ[Math.floor(vertices.length * 0.1)].z,
                z50: byZ[Math.floor(vertices.length * 0.5)].z,
                z90: byZ[Math.floor(vertices.length * 0.9)].z,
                
                minDist: Math.sqrt(byDistance[0].x**2 + byDistance[0].y**2 + byDistance[0].z**2),
                medianDist: Math.sqrt(
                    byDistance[Math.floor(vertices.length * 0.5)].x**2 +
                    byDistance[Math.floor(vertices.length * 0.5)].y**2 +
                    byDistance[Math.floor(vertices.length * 0.5)].z**2
                ),
                maxDist: Math.sqrt(
                    byDistance[vertices.length - 1].x**2 +
                    byDistance[vertices.length - 1].y**2 +
                    byDistance[vertices.length - 1].z**2
                ),
                
                // spread is cool
                xSpread: byX[byX.length - 1].x - byX[0].x,
                ySpread: byY[byY.length - 1].y - byY[0].y,
                zSpread: byZ[byZ.length - 1].z - byZ[0].z,
                
                centerX: vertices.reduce((sum, v) => sum + v.x, 0) / vertices.length,
                centerY: vertices.reduce((sum, v) => sum + v.y, 0) / vertices.length,
                centerZ: vertices.reduce((sum, v) => sum + v.z, 0) / vertices.length,
                
                variance: this.calculateVariance(vertices)
            };
            
            return stats;
        }
        
        static calculateVariance(vertices) {
            const mean = {
                x: vertices.reduce((sum, v) => sum + v.x, 0) / vertices.length,
                y: vertices.reduce((sum, v) => sum + v.y, 0) / vertices.length,
                z: vertices.reduce((sum, v) => sum + v.z, 0) / vertices.length
            };
            
            const variance = vertices.reduce((sum, v) => {
                return sum + 
                    Math.pow(v.x - mean.x, 2) +
                    Math.pow(v.y - mean.y, 2) +
                    Math.pow(v.z - mean.z, 2);
            }, 0) / vertices.length;
            
            return variance;
        }
        
        static mapToAudioParams(stats) {
            const normalize = (value, min, max) => {
                return Math.max(0, Math.min(1, (value - min) / (max - min)));
            };
            
            return {
                reverbSize: normalize(stats.medianDist, 0, 10),
                reverbDamping: normalize(stats.variance, 0, 5),
                reverbWetness: normalize(stats.ySpread, 0, 5) * 0.5,
                
                delayTime: normalize(stats.zSpread, 0, 5) * 0.5, // 0-500ms
                delayFeedback: normalize(Math.abs(stats.centerZ), 0, 3) * 0.7,
                
                filterFreq: 200 + normalize(stats.y90, -5, 5) * 3800, // 200-4000 Hz
                filterQ: 1 + normalize(stats.xSpread, 0, 5) * 10,
                
                distortionAmount: normalize(stats.variance, 0, 10) * 50,
                
                pan: Math.max(-1, Math.min(1, stats.centerX / 5)),
                
                chorusDepth: normalize(stats.x90 - stats.x10, 0, 5),
                chorusRate: 0.5 + normalize(stats.minDist, 0, 2) * 4,

                // add spectral_mash
                spectral_mash: normalize(stats.variance, 0, 10)

            };
        }
    }
    
    async function spectral_mash(inputBuffer, meshStats){
      const sampleRate = inputBuffer.sampleRate;
      const input = inputBuffer.getChannelData(0);
      const output = new Float32Array(input.length);

      const fftSize = 4096;
      const fft = new FFT(fftSize, sampleRate);

      for (let i = 0; i < input.length - fftSize; i += fftSize/2){
        // i forgot slice existed.
        const chunk = input.slice(i,i + fftSize);

        fft.forward(chunk);

        const real = fft.real;
        const imag = fft.imag;
        // mesh calc
        // the actuall PROCESSING HAPPENS HERE. PUT ALL YOUR SPECTRAL DSP IN HERE.
        // i literally know ZERO spectral dsp other than vocoding and de-harm
        // i guess i'll just make stuff up then.
        for (let bin = 0; bin < real.length; bin++){
          const freq = bin * sampleRate / fftSize;
          const gain = 1 + meshStats.variance * (freq / 1000);
          real[bin] *= gain;
          imag[bin] *= gain;
        }

        const processed = fft.inverse(real,imag);

        for(let j = 0; j < processed.length; j++){
          if (i + j < output.length){
            output[i + j] += processed[j] * 0.5;
          }
        }
      }
      return output;
    }

    class AudioProcessor {
        constructor() {
            this.audioContext = null;
            this.sourceBuffer = null;
            this.processedBuffer = null;
            this.isPlaying = false;
            this.currentSource = null;
        }
        
        async init() {
            this.audioContext = new (window.AudioContext || window.webkitAudioContext)();
        }
        
        async loadAudioFile(file) {
            const arrayBuffer = await file.arrayBuffer();
            this.sourceBuffer = await this.audioContext.decodeAudioData(arrayBuffer);
            return this.sourceBuffer;
        }
        
        async processWithParameters(params) {
            if (!this.sourceBuffer) {
                throw new Error('No audio loaded');
            }

            let processedBuffer = this.sourceBuffer;
            if(params.spectral_mash > 0){
              const spectralData = await spectral_mash(this.sourceBuffer, params);
              
              processedBuffer = this.audioContext.createBuffer(1,
              spectralData.length,
              this.sourceBuffer.sampleRate
              );
              processedBuffer.copyToChannel(spectralData,0);
            }
            
            const offlineContext = new OfflineAudioContext(
                this.sourceBuffer.numberOfChannels,
                this.sourceBuffer.length,
                this.sourceBuffer.sampleRate
            );
            
            const source = offlineContext.createBufferSource();
            // changed to processed buffer. used to be this.sourceBuffer.
            // just remember for if you ever you offlie audio context ever again.
            source.buffer = processedBuffer;
            
            let currentNode = source;
            
            if (params.distortionAmount > 0) {
                const distortion = offlineContext.createWaveShaper();
                distortion.curve = this.makeDistortionCurve(params.distortionAmount);
                distortion.oversample = '4x';
                currentNode.connect(distortion);
                currentNode = distortion;
            }
            
            const filter = offlineContext.createBiquadFilter();
            filter.type = 'lowpass';
            filter.frequency.value = params.filterFreq;
            filter.Q.value = params.filterQ;
            currentNode.connect(filter);
            currentNode = filter;
            
            const delay = offlineContext.createDelay(1.0);
            const delayGain = offlineContext.createGain();
            const delayFeedback = offlineContext.createGain();
            const delayWet = offlineContext.createGain();
            
            delay.delayTime.value = params.delayTime;
            delayFeedback.gain.value = params.delayFeedback;
            delayWet.gain.value = 0.3;
            
            currentNode.connect(delay);
            delay.connect(delayGain);
            delayGain.connect(delayFeedback);
            delayFeedback.connect(delay);
            delayGain.connect(delayWet);
            
            const convolver = offlineContext.createConvolver();
            convolver.buffer = this.createReverbImpulse(
                offlineContext.sampleRate,
                params.reverbSize * 4,
                params.reverbDamping
            );
            
            const reverbWet = offlineContext.createGain();
            reverbWet.gain.value = params.reverbWetness;
            
            currentNode.connect(convolver);
            convolver.connect(reverbWet);
            
            const dryGain = offlineContext.createGain();
            dryGain.gain.value = 0.7;
            currentNode.connect(dryGain);
            
            const panner = offlineContext.createStereoPanner();
            panner.pan.value = params.pan;
            
            dryGain.connect(panner);
            delayWet.connect(panner);
            reverbWet.connect(panner);
            panner.connect(offlineContext.destination);

          // okay i think offlineAudioContext works right here ish. idk confusing.
            source.start(0);
            
            this.processedBuffer = await offlineContext.startRendering();
            return this.processedBuffer;
        }
        
        makeDistortionCurve(amount) {
            const samples = 44100;
            const curve = new Float32Array(samples);
            const deg = Math.PI / 180;
            
            for (let i = 0; i < samples; i++) {
                const x = (i * 2) / samples - 1;
                curve[i] = ((3 + amount) * x * 20 * deg) / (Math.PI + amount * Math.abs(x));
            }
            
            return curve;
        }
        
        createReverbImpulse(sampleRate, duration, decay) {
            const length = sampleRate * duration;
            const impulse = new Float32Array(length);
            
            for (let i = 0; i < length; i++) {
                impulse[i] = (Math.random() * 2 - 1) * Math.pow(1 - i / length, decay);
            }
            
            return this.audioContext.createBuffer(1, length, sampleRate);
        }
        
        play(buffer) {
            if (this.isPlaying) this.stop();
            
            this.currentSource = this.audioContext.createBufferSource();
            this.currentSource.buffer = buffer || this.processedBuffer;
            this.currentSource.connect(this.audioContext.destination);
            this.currentSource.start(0);
            this.isPlaying = true;
            
            this.currentSource.onended = () => {
                this.isPlaying = false;
            };
        }
        
        stop() {
            if (this.currentSource) {
                this.currentSource.stop();
                this.currentSource = null;
                this.isPlaying = false;
            }
        }
        
        downloadProcessedAudio(filename = 'processed.wav') {
            if (!this.processedBuffer) {
                throw new Error('No processed audio available');
            }
            
            const wav = this.audioBufferToWav(this.processedBuffer);
            const blob = new Blob([wav], { type: 'audio/wav' });
            const url = URL.createObjectURL(blob);
            
            const a = document.createElement('a');
            a.href = url;
            a.download = filename;
            a.click();
            
            URL.revokeObjectURL(url);
        }
        
        audioBufferToWav(buffer) {
            const numChannels = buffer.numberOfChannels;
            const sampleRate = buffer.sampleRate;
            const format = 1; // PCM
            const bitDepth = 16;
            
            const bytesPerSample = bitDepth / 8;
            const blockAlign = numChannels * bytesPerSample;
            
            const data = [];
            for (let i = 0; i < buffer.numberOfChannels; i++) {
                data.push(buffer.getChannelData(i));
            }
            
            const length = data[0].length;
            const arrayBuffer = new ArrayBuffer(44 + length * blockAlign);
            const view = new DataView(arrayBuffer);
            
            // WAV header
            const writeString = (offset, string) => {
                for (let i = 0; i < string.length; i++) {
                    view.setUint8(offset + i, string.charCodeAt(i));
                }
            };
            
            writeString(0, 'RIFF');
            view.setUint32(4, 36 + length * blockAlign, true);
            writeString(8, 'WAVE');
            writeString(12, 'fmt ');
            view.setUint32(16, 16, true);
            view.setUint16(20, format, true);
            view.setUint16(22, numChannels, true);
            view.setUint32(24, sampleRate, true);
            view.setUint32(28, sampleRate * blockAlign, true);
            view.setUint16(32, blockAlign, true);
            view.setUint16(34, bitDepth, true);
            writeString(36, 'data');
            view.setUint32(40, length * blockAlign, true);
            
            // Convert float samples to PCM
            let offset = 44;
            for (let i = 0; i < length; i++) {
                for (let channel = 0; channel < numChannels; channel++) {
                    const sample = Math.max(-1, Math.min(1, data[channel][i]));
                    view.setInt16(offset, sample * 0x7FFF, true);
                    offset += 2;
                }
            }
            
            return arrayBuffer;
        }
    }
    
    // ui control
    const audioProcessor = new AudioProcessor();
    let meshFingerprint = null;
    let audioParams = null;
    
    // start processing when interact`
    document.addEventListener('click', () => {
        if (!audioProcessor.audioContext) {
            audioProcessor.init();
        }
    }, { once: true });
    
    document.getElementById('obj-input').addEventListener('change', async (e) => {
        const file = e.target.files[0];
        if (file) {
            const text = await file.text();
            const vertices = OBJParser.parse(text);
            
            document.getElementById('status-text').textContent = `Loaded ${vertices.length} vertices`;

            // here here here
            // TODO: uhhhh
            // lowkey this solution is kinda stupid because 
            // its like creating a audioParams object just for display???
            // i'm kinda addicted to vibe coding but whateva :3
            // there should be an object that contains the normalized values but also the display text.
            // i'll fix that later when i document all this.
            // like its pissing me off that i create meshFingerprint then pass it to audio params
            // thats an unecessary amount of memory usage.
            meshFingerprint = MeshFingerprint.calculate(vertices);
            audioParams = MeshFingerprint.mapToAudioParams(meshFingerprint);
            
            const paramsDisplay = document.getElementById('params-display');
            const paramsList = document.getElementById('params-list');
            paramsDisplay.style.display = 'block';
            
            paramsList.innerHTML = Object.entries(audioParams)
                .map(([key, value]) => `<p>${key}: ${value.toFixed(3)}</p>`)
                .join('');
            
            if (audioProcessor.sourceBuffer) {
                document.getElementById('process-btn').disabled = false;
            }
        }
    });
    
    document.getElementById('wav-input').addEventListener('change', async (e) => {
        const file = e.target.files[0];
        if (file) {
            await audioProcessor.init();
            await audioProcessor.loadAudioFile(file);
            
            document.getElementById('status-text').textContent = 'Audio loaded';
            
            if (audioParams) {
                document.getElementById('process-btn').disabled = false;
            }
        }
    });
    
    document.getElementById('process-btn').addEventListener('click', async () => {
        if (audioParams && audioProcessor.sourceBuffer) {
            document.getElementById('status-text').textContent = 'Processing...';
            
            try {
                await audioProcessor.processWithParameters(audioParams);
                
                document.getElementById('status-text').textContent = 'Processing complete!';
                document.getElementById('play-btn').disabled = false;
                document.getElementById('download-btn').disabled = false;
            } catch (error) {
                document.getElementById('status-text').textContent = `Error: ${error.message}`;
            }
        }
    });
    
    document.getElementById('play-btn').addEventListener('click', () => {
        audioProcessor.play();
        document.getElementById('stop-btn').disabled = false;
    });
    
    document.getElementById('stop-btn').addEventListener('click', () => {
        audioProcessor.stop();
        document.getElementById('stop-btn').disabled = true;
    });
    
    document.getElementById('download-btn').addEventListener('click', () => {
        audioProcessor.downloadProcessedAudio('dimensionality-processed.wav');
    });
    </script>
</body>
</html>
